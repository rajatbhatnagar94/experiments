{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, BertConfig, BertForSequenceClassification\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "datasets = [\n",
    "     {\n",
    "         \"name\": \"Stanford treebank\",\n",
    "         \"prefix\": \"stanford_treebank\",\n",
    "         \"train_path\": \"/data/sam/stanford_treebank/sst_train.csv\",\n",
    "         \"dev_path\": \"/data/sam/stanford_treebank/sst_dev.csv\",\n",
    "         \"test_path\": \"/data/sam/stanford_treebank/sst_test.csv\",\n",
    "         'classes': ['neg', 'pos']\n",
    "     },\n",
    "#    {\n",
    "#        \"name\": \"Reddit Dataset\",\n",
    "#        \"prefix\": \"reddit_dataset\",\n",
    "#        \"train_path\": \"/data/rajat/ex_ml/reddit/input/train.csv\",\n",
    "#        \"dev_path\": \"/data/rajat/ex_ml/reddit/input/dev.csv\",\n",
    "#        \"test_path\": \"/data/rajat/ex_ml/reddit/input/test.csv\",\n",
    "#        'classes': [0, 1]\n",
    "#    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "867    0\n",
       "868    0\n",
       "869    0\n",
       "870    0\n",
       "871    0\n",
       "Name: classification, Length: 872, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_dir_name(parameters):\n",
    "    return './outputs_{}_lr={}_epochs={}'.format(datasets[0]['prefix'], parameters['learning_rate'], epochs)\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_csv(datasets[0]['train_path'],index_col=0)\n",
    "    dev = pd.read_csv(datasets[0]['dev_path'],index_col=0)\n",
    "    test = pd.read_csv(datasets[0]['test_path'],index_col=0)\n",
    "    return train, dev, test\n",
    "train_data, dev, test = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data, tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'), **kwargs):\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    try:\n",
    "        for x in data:\n",
    "            tokenized_x = tokenizer.encode_plus(x,\n",
    "                                                max_length=128,\n",
    "                                                add_special_tokens = True,\n",
    "                                                pad_to_max_length=True,\n",
    "                                                padding_side='right',\n",
    "                                                return_token_type_ids=True,\n",
    "                                                return_attention_mask=True)\n",
    "            input_ids.append(tokenized_x['input_ids'])\n",
    "            token_type_ids.append(tokenized_x['token_type_ids'])\n",
    "            attention_mask.append(tokenized_x['attention_mask'])\n",
    "    except Exception as e:\n",
    "        print(e, x)\n",
    "    return torch.tensor(input_ids, dtype=torch.long), torch.tensor(token_type_ids, dtype=torch.long), torch.tensor(attention_mask, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(df, batch_size=4, **kwargs):\n",
    "    x, y = list(df['text'].values), torch.tensor(list(df['classification'].apply(lambda y: datasets[0]['classes'].index(y))), device=device, dtype=torch.long)\n",
    "    input_ids, token_type_ids, attention_mask = encode(x, **kwargs)\n",
    "    tensor_dataset = torch.utils.data.TensorDataset(input_ids, token_type_ids, attention_mask, y)\n",
    "    tensor_randomsampler = torch.utils.data.RandomSampler(tensor_dataset)\n",
    "    tensor_dataloader = torch.utils.data.DataLoader(tensor_dataset, sampler=tensor_randomsampler, batch_size=batch_size)\n",
    "    return tensor_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch, epochs=2):\n",
    "    model.train()\n",
    "    for e in range(epochs):\n",
    "        for i, batch_tuple in enumerate(batch):\n",
    "            batch_tuple = (t.to(device) for t in batch_tuple)\n",
    "            input_ids, token_type_ids, attention_mask, labels = batch_tuple\n",
    "            outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss, logits, hidden_states_output, attention_mask_output = outputs\n",
    "            if i % 100 == 0:\n",
    "                print(\"loss - {0}\".format(loss))\n",
    "            model.zero_grad()        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), parameters['max_grad_norm'])\n",
    "            optimizer.step()\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_dev = get_batches(dev, batch_size=1, tokenizer=tokenizer)\n",
    "#batch_train = get_batches(train_data, batch_size=8, tokenizer=tokenizer)\n",
    "#batch_test = get_batches(test, batch_size=1, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "batch_dev = get_batches(dev, batch_size=1, tokenizer=tokenizer)\n",
    "batch_train = get_batches(train_data, batch_size=8, tokenizer=tokenizer)\n",
    "batch_test = get_batches(test, batch_size=1, tokenizer=tokenizer)\n",
    "\n",
    "epochs=2\n",
    "parameters = {\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_warmup_steps': 1000,\n",
    "    'num_training_steps': len(batch_train) * epochs,\n",
    "    'max_grad_norm': 1,\n",
    "    'epochs': epochs\n",
    "}\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, output_hidden_states=True, output_attentions=True)\n",
    "#model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True, output_attentions=True)\n",
    "model.to(device)\n",
    "optimizer = transformers.AdamW(model.parameters(), lr=parameters['learning_rate'], correct_bias=False)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer,\n",
    "                                                         num_warmup_steps=parameters['num_warmup_steps'],\n",
    "                                                         num_training_steps=parameters['num_training_steps'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(batch):\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for i, batch_cpu in enumerate(batch):\n",
    "        batch_gpu = (t.to(device) for t in batch_cpu)\n",
    "        input_ids, token_type_ids, attention_mask, labels = batch_gpu\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss, logits, hidden_states_output, attention_mask_output = outputs\n",
    "            loss = loss.detach().cpu()\n",
    "            logits =  logits.detach().cpu()\n",
    "            labels_cpu = labels.detach().cpu()\n",
    "            input_ids_cpu = input_ids.detach().cpu()\n",
    "#             hidden_states_output = tuple(t.detach().cpu() for t in hidden_states_output)\n",
    "#             attention_mask_output = tuple(t.detach().cpu() for t in attention_mask_output)\n",
    "#             results.append(tuple([loss, logits, hidden_states_output, attention_mask_output]))\n",
    "            results.append(tuple([loss, logits, labels_cpu, input_ids_cpu, None, None]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss - 0.7048430442810059\n",
      "loss - 0.6011022329330444\n",
      "loss - 0.3259471654891968\n",
      "loss - 0.09624850004911423\n",
      "loss - 0.33943769335746765\n",
      "loss - 0.1105036810040474\n",
      "loss - 0.013818517327308655\n",
      "loss - 0.45614898204803467\n",
      "loss - 0.2383934110403061\n",
      "loss - 0.005538195371627808\n",
      "loss - 0.004141062498092651\n",
      "loss - 0.03702165186405182\n",
      "loss - 0.007148325443267822\n",
      "loss - 0.8673495054244995\n",
      "loss - 0.003480762243270874\n",
      "loss - 0.0038371384143829346\n",
      "loss - 0.005284488201141357\n",
      "loss - 0.0041169822216033936\n"
     ]
    }
   ],
   "source": [
    "train(batch_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(results):\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    count = 0\n",
    "    for result in results:\n",
    "        loss, logits, labels, input_ids, hidden_states_output, attention_mask_output = result\n",
    "        prediction = torch.argmax(logits, dim=1).tolist()\n",
    "        if prediction[0] != labels[0]:\n",
    "            count += 1\n",
    "        predictions += prediction\n",
    "        true_labels += labels.tolist()\n",
    "    print(\"wrong: {}\".format(count))\n",
    "    return true_labels, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(batch_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [0] [CLS] treasure is fucking you [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[1] [0] [CLS] stop whining chris [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[1] [0] [CLS] federer is such a fucking cool guy [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[1] [0] [CLS] long ass noodle arm [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[1] [0] [CLS] is fucking you a treasure [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[1] [0] [CLS] gay [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "wrong: 7\n",
      "[1.         0.99733215] [0.99729851 1.        ] [0.99864743 0.99866429] [2221 2243]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     1.0000    0.9973    0.9986      2221\n",
      "    positive     0.9973    1.0000    0.9987      2243\n",
      "\n",
      "    accuracy                         0.9987      4464\n",
      "   macro avg     0.9987    0.9986    0.9987      4464\n",
      "weighted avg     0.9987    0.9987    0.9987      4464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = get_prediction(results)\n",
    "target_names = [\"negative\", \"positive\"]\n",
    "classification = confusion_matrix(y_true, y_pred)\n",
    "precision, recall, fbeta_scorefloat, support = precision_recall_fscore_support(y_true, y_pred)\n",
    "classification = classification_report(y_true, y_pred, target_names=target_names, digits=4)\n",
    "print(precision, recall, fbeta_scorefloat, support)\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [0] [CLS] not to sound racist but seeing black people portrayed this was makes me so happy than how the news portrays black people [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[1] [0] [CLS] lol what a shit [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[1] [0] [CLS] fuck dem haters bro [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[0] [1] [CLS] all this subreddit is bunch of kids crying [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[0] [1] [CLS] bitch please [UNK] [UNK] [UNK] a [UNK] [UNK] [UNK] [UNK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[1] [0] [CLS] ya alcohol will punish you no doubt lol [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[1] [0] [CLS] # get the fuck back here you little shit [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[1] [0] [CLS] check out everyone who is starting tomorrow because of you posting this [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[1] [0] [CLS] yeah shagga this is aussie as fuck [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[1] [0] [CLS] your ass is very wise sir [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "wrong: 11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9991    0.9964    0.9978      2226\n",
      "    positive     0.9964    0.9991    0.9978      2238\n",
      "\n",
      "    accuracy                         0.9978      4464\n",
      "   macro avg     0.9978    0.9978    0.9978      4464\n",
      "weighted avg     0.9978    0.9978    0.9978      4464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(batch_test)\n",
    "y_true, y_pred = get_prediction(results)\n",
    "target_names = [\"negative\", \"positive\"]\n",
    "classification = classification_report(y_true, y_pred, target_names=target_names, digits=4)\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save():\n",
    "    output_dir = get_output_dir_name(parameters)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    print(\"Saving model to {}\".format(output_dir))\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to {0} ./outputs_reddit_lr=2e-05_epochs=2\n"
     ]
    }
   ],
   "source": [
    "save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Stanford Treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong: 79\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9051    0.9136    0.9093       428\n",
      "    positive     0.9159    0.9077    0.9118       444\n",
      "\n",
      "    accuracy                         0.9106       872\n",
      "   macro avg     0.9105    0.9106    0.9105       872\n",
      "weighted avg     0.9106    0.9106    0.9106       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(batch_dev)\n",
    "y_true, y_pred = get_prediction(results)\n",
    "target_names = [\"negative\", \"positive\"]\n",
    "classification = classification_report(y_true, y_pred, target_names=target_names, digits=4)\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong: 152\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9242    0.9090    0.9165       912\n",
      "    positive     0.9102    0.9252    0.9176       909\n",
      "\n",
      "    accuracy                         0.9171      1821\n",
      "   macro avg     0.9172    0.9171    0.9171      1821\n",
      "weighted avg     0.9172    0.9171    0.9171      1821\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(batch_test)\n",
    "y_true, y_pred = get_prediction(results)\n",
    "target_names = [\"negative\", \"positive\"]\n",
    "classification = classification_report(y_true, y_pred, target_names=target_names, digits=4)\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./outputs_stanford_treebank_lr=2e-05_epochs=2\n"
     ]
    }
   ],
   "source": [
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>text</th>\n",
       "      <th>classification</th>\n",
       "      <th>rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>the rock is destined to be the 21st century 's...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>the gorgeously elaborate continuation of `` th...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "      <td>singer/composer bryan adams contributes a slew...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>yet the act is still charming here .</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Whether or not you 're enlightened by any of D...</td>\n",
       "      <td>whether or not you 're enlightened by any of d...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>8539</td>\n",
       "      <td>A real snooze .</td>\n",
       "      <td>a real snooze .</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.0, 0.5, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>8540</td>\n",
       "      <td>No surprises .</td>\n",
       "      <td>no surprises .</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.5, 0.5, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>8541</td>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>we 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>8542</td>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "      <td>her fans walked out muttering words like `` ho...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>8543</td>\n",
       "      <td>In this case zero .</td>\n",
       "      <td>in this case zero .</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.5, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                      original_text  \\\n",
       "0        0  The Rock is destined to be the 21st Century 's...   \n",
       "1        1  The gorgeously elaborate continuation of `` Th...   \n",
       "2        2  Singer/composer Bryan Adams contributes a slew...   \n",
       "3        4               Yet the act is still charming here .   \n",
       "4        5  Whether or not you 're enlightened by any of D...   \n",
       "...    ...                                                ...   \n",
       "6915  8539                                    A real snooze .   \n",
       "6916  8540                                     No surprises .   \n",
       "6917  8541  We 've seen the hippie-turned-yuppie plot befo...   \n",
       "6918  8542  Her fans walked out muttering words like `` ho...   \n",
       "6919  8543                                In this case zero .   \n",
       "\n",
       "                                                   text classification  \\\n",
       "0     the rock is destined to be the 21st century 's...            pos   \n",
       "1     the gorgeously elaborate continuation of `` th...            pos   \n",
       "2     singer/composer bryan adams contributes a slew...            pos   \n",
       "3                  yet the act is still charming here .            pos   \n",
       "4     whether or not you 're enlightened by any of d...            pos   \n",
       "...                                                 ...            ...   \n",
       "6915                                    a real snooze .            neg   \n",
       "6916                                     no surprises .            neg   \n",
       "6917  we 've seen the hippie-turned-yuppie plot befo...            pos   \n",
       "6918  her fans walked out muttering words like `` ho...            neg   \n",
       "6919                                in this case zero .            neg   \n",
       "\n",
       "                                              rationale  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1     [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3              [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]  \n",
       "4     [0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, ...  \n",
       "...                                                 ...  \n",
       "6915                               [0.0, 0.5, 1.0, 0.0]  \n",
       "6916                                    [0.5, 0.5, 0.0]  \n",
       "6917  [0.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.0, 0.0, ...  \n",
       "6918  [0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
       "6919                          [0.0, 0.0, 0.0, 0.5, 0.0]  \n",
       "\n",
       "[6920 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>text</th>\n",
       "      <th>classification</th>\n",
       "      <th>rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>It 's a lovely film with lovely performances b...</td>\n",
       "      <td>it 's a lovely film with lovely performances b...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>And if you 're not nearly moved to tears by a ...</td>\n",
       "      <td>and if you 're not nearly moved to tears by a ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A warm , funny , engaging film .</td>\n",
       "      <td>a warm , funny , engaging film .</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Uses sharp humor and insight into human nature...</td>\n",
       "      <td>uses sharp humor and insight into human nature...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Entertains by providing good , lively company .</td>\n",
       "      <td>entertains by providing good , lively company .</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.5, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>1095</td>\n",
       "      <td>... Designed to provide a mix of smiles and te...</td>\n",
       "      <td>... designed to provide a mix of smiles and te...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>1096</td>\n",
       "      <td>it seems to me the film is about the art of ri...</td>\n",
       "      <td>it seems to me the film is about the art of ri...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>1097</td>\n",
       "      <td>It 's just disappointingly superficial -- a mo...</td>\n",
       "      <td>it 's just disappointingly superficial -- a mo...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>1098</td>\n",
       "      <td>The title not only describes its main characte...</td>\n",
       "      <td>the title not only describes its main characte...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>1100</td>\n",
       "      <td>Schaeffer has to find some hook on which to ha...</td>\n",
       "      <td>schaeffer has to find some hook on which to ha...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>872 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                      original_text  \\\n",
       "0       0  It 's a lovely film with lovely performances b...   \n",
       "1       2  And if you 're not nearly moved to tears by a ...   \n",
       "2       3                   A warm , funny , engaging film .   \n",
       "3       4  Uses sharp humor and insight into human nature...   \n",
       "4       6    Entertains by providing good , lively company .   \n",
       "..    ...                                                ...   \n",
       "867  1095  ... Designed to provide a mix of smiles and te...   \n",
       "868  1096  it seems to me the film is about the art of ri...   \n",
       "869  1097  It 's just disappointingly superficial -- a mo...   \n",
       "870  1098  The title not only describes its main characte...   \n",
       "871  1100  Schaeffer has to find some hook on which to ha...   \n",
       "\n",
       "                                                  text classification  \\\n",
       "0    it 's a lovely film with lovely performances b...            pos   \n",
       "1    and if you 're not nearly moved to tears by a ...            pos   \n",
       "2                     a warm , funny , engaging film .            pos   \n",
       "3    uses sharp humor and insight into human nature...            pos   \n",
       "4      entertains by providing good , lively company .            pos   \n",
       "..                                                 ...            ...   \n",
       "867  ... designed to provide a mix of smiles and te...            neg   \n",
       "868  it seems to me the film is about the art of ri...            neg   \n",
       "869  it 's just disappointingly superficial -- a mo...            neg   \n",
       "870  the title not only describes its main characte...            neg   \n",
       "871  schaeffer has to find some hook on which to ha...            neg   \n",
       "\n",
       "                                             rationale  \n",
       "0    [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...  \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, ...  \n",
       "2             [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]  \n",
       "3    [0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, ...  \n",
       "4             [0.5, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0]  \n",
       "..                                                 ...  \n",
       "867  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "868  [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "869  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "870  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[872 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>text</th>\n",
       "      <th>classification</th>\n",
       "      <th>rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 0.5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>offers that rare combination of entertainment ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perhaps no picture ever made has more literall...</td>\n",
       "      <td>perhaps no picture ever made has more literall...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Steers turns in a snappy screenplay that curls...</td>\n",
       "      <td>steers turns in a snappy screenplay that curls...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>2205</td>\n",
       "      <td>An imaginative comedy/thriller .</td>\n",
       "      <td>an imaginative comedy/thriller .</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.0, 1.0, 0.5, 0.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>2206</td>\n",
       "      <td>( A ) rare , beautiful film .</td>\n",
       "      <td>( a ) rare , beautiful film .</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.5, 0.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>2207</td>\n",
       "      <td>( An ) hilarious romantic comedy .</td>\n",
       "      <td>( an ) hilarious romantic comedy .</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.5, 0.0, 0.5, 1.0, 0.0, 0.5, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>2208</td>\n",
       "      <td>Never ( sinks ) into exploitation .</td>\n",
       "      <td>never ( sinks ) into exploitation .</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.5, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>2209</td>\n",
       "      <td>( U ) nrelentingly stupid .</td>\n",
       "      <td>( u ) nrelentingly stupid .</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.5, 0.0, 0.5, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                      original_text  \\\n",
       "0        1  If you sometimes like to go to the movies to h...   \n",
       "1        2  Emerges as something rare , an issue movie tha...   \n",
       "2        4  Offers that rare combination of entertainment ...   \n",
       "3        5  Perhaps no picture ever made has more literall...   \n",
       "4        6  Steers turns in a snappy screenplay that curls...   \n",
       "...    ...                                                ...   \n",
       "1816  2205                   An imaginative comedy/thriller .   \n",
       "1817  2206                      ( A ) rare , beautiful film .   \n",
       "1818  2207                 ( An ) hilarious romantic comedy .   \n",
       "1819  2208                Never ( sinks ) into exploitation .   \n",
       "1820  2209                        ( U ) nrelentingly stupid .   \n",
       "\n",
       "                                                   text classification  \\\n",
       "0     if you sometimes like to go to the movies to h...            pos   \n",
       "1     emerges as something rare , an issue movie tha...            pos   \n",
       "2     offers that rare combination of entertainment ...            pos   \n",
       "3     perhaps no picture ever made has more literall...            pos   \n",
       "4     steers turns in a snappy screenplay that curls...            pos   \n",
       "...                                                 ...            ...   \n",
       "1816                   an imaginative comedy/thriller .            pos   \n",
       "1817                      ( a ) rare , beautiful film .            pos   \n",
       "1818                 ( an ) hilarious romantic comedy .            pos   \n",
       "1819                never ( sinks ) into exploitation .            pos   \n",
       "1820                        ( u ) nrelentingly stupid .            neg   \n",
       "\n",
       "                                              rationale  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 0.5, ...  \n",
       "1     [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2         [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]  \n",
       "3     [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, ...  \n",
       "4     [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                 ...  \n",
       "1816                               [0.0, 1.0, 0.5, 0.5]  \n",
       "1817           [0.5, 0.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.0]  \n",
       "1818                [0.5, 0.0, 0.5, 1.0, 0.0, 0.5, 0.0]  \n",
       "1819                [0.5, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0]  \n",
       "1820                     [0.5, 0.0, 0.5, 0.0, 1.0, 0.0]  \n",
       "\n",
       "[1821 rows x 5 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
